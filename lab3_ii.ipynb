{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторная работа 2. Обработка текста.\n",
    "\n",
    "Задание:\n",
    "\n",
    "Написать программу на языке программирования Python, реализующую:\n",
    "1)\tПредварительную обработку текста, включающую:\n",
    "\t-\tПеревод текста в нижний регистр\n",
    "\t-\tУдаление знаков препинания\n",
    "\t-\tУдаление стоп-слов\n",
    "\t-\tУдаление лишних символов\n",
    "\t-\tСтемминг и лемматизацию\n",
    "2)\tПреобразование текста в мешок слов\n",
    "3)\tПреобразование текста в N-граммы, где N=2,4\n",
    "\n",
    "Замечание!\n",
    "Желательно провести предварительную обработку текста с использованием библиотеки NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer # Стеммер для русского языка\n",
    "from nltk.stem.porter import PorterStemmer # Стеммер для английского \n",
    "from nltk.stem import WordNetLemmatizer # Лемматайзер для английского\n",
    "from nltk.corpus import wordnet\n",
    "from pymystem3 import Mystem # Лемматайзер для русского языка \n",
    "import re\n",
    "import string\n",
    "from langdetect import detect\n",
    "import logging\n",
    "import os \n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "from num2words import num2words\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Конфигурационный блок \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = r'D:\\Projects\\SvetlanaDmitrievna\\lab3_ii\\logs'\n",
    "os.makedirs(log_dir, exist_ok=True) \n",
    "log_filename = os.path.join(log_dir, f'{datetime.now().strftime(\"%H %M %S %Y %m %d\")} nlp.log')\n",
    "\n",
    "# Удаляем логеры, которые уже запущены по случайности \n",
    "logger = logging.getLogger()\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "    \n",
    "# Настройки для логгера \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # Вывод в консоль \n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:160: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:160: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\vadig\\AppData\\Local\\Temp\\ipykernel_3048\\56961097.py:160: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  '''\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessing: \n",
    "\t\"\"\"\n",
    "\tДелает упорядоченную предобработку текста: \n",
    "\t1. Перевод текста в нижний регистр. \n",
    "\t2. Удаление пунтуации и других символов. \n",
    "\t3. Удаление стоп-слов. \n",
    "\t4. Нормализация текста. Лемматизация / стемминг. \n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, text, delete_punctuation_mode='string', text_normalization_method='lemma'):\n",
    "\t\t\"\"\"\n",
    "\t\tИнициализация класса TextPreprocessing.\n",
    "\n",
    "\t\t:param text: исходный текст для предобработки\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Text is initializing...')\n",
    "\n",
    "\t\tself.text = text\n",
    "\t\tself.delete_punctuation_mode = delete_punctuation_mode\n",
    "\t\tself.language = self.__detect_language() \n",
    "\t\tself.normalization_method = text_normalization_method\n",
    "\n",
    "\t\tlogger.info(f'Text is initialized with parameters: delete_punctuation_mode={delete_punctuation_mode}, text_normalization_method={text_normalization_method}')\n",
    " \n",
    "\tdef __detect_language(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tАвтоматическое определение языка с помощью библиотеки langdetect. \n",
    "\n",
    "\t\t:return: Возвращается формат языка в стиле 'russian', 'english'.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tdetected_lang = detect(self.text)\n",
    "\t\t\t\n",
    "\t\t\tlang_mapping = {\n",
    "\t\t\t\t'ru': 'russian',\n",
    "\t\t\t\t'en': 'english',\n",
    "\t\t\t\t# TODO: create more languages\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tlanguage = lang_mapping[detected_lang]\n",
    "\n",
    "\t\t\tlogger.info(f'Detected language: {language}')\n",
    "\t\t\t\n",
    "\t\t\treturn language\n",
    "\t\t\n",
    "\t\texcept:\n",
    "\t\t\tlogger.critical('Error with detecting langauge.')\n",
    "\n",
    "\tdef __lang_to_simple_form(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tВовзвращает упрощенную форму языка. \n",
    "\n",
    "\t\t'english' -> 'en' \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlang_mapping = {\n",
    "\t\t\t\t'russian': 'ru', \n",
    "\t\t\t\t'english': 'en'\n",
    "\t\t\t}\n",
    "\t\t\n",
    "\t\treturn lang_mapping[self.language]\n",
    " \n",
    "\tdef __to_lower_case(self) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tПриводит текст к нижнему регистру. \n",
    "\n",
    "\t\t:return: возвращаем измененный текст. \n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tlogger.info('Text has been converted to lower case.')\n",
    "\n",
    "\t\treturn self.text.lower()\n",
    "\t\n",
    "\tdef __correct_spell(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tИсправляет опечатки в тексте. \n",
    "\t\t\n",
    "\t\t:param text: исходный текст.\n",
    "    :return: текст с исправленными опечатками.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Correction of typos initiated...')\n",
    "\n",
    "\t\ttry:\n",
    "\n",
    "\t\t\tif self.language == 'english':\n",
    "\t\t\t\tspell = SpellChecker(language=self.__lang_to_simple_form()) \n",
    "\t\t\t\twords = self.text.split() \n",
    "\t\t\t\tcorrected_words = [] \n",
    "\t\t\t\tfor word in words: \n",
    "\n",
    "\t\t\t\t\tif word.isdigit() or len(word) <= 2: \n",
    "\t\t\t\t\t\tcorrected_words.append(word)\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tcorrected_word = spell.correction(word)\n",
    "\t\t\t\t\tcorrected_words.append(corrected_word if corrected_word else word)\n",
    "\n",
    "\t\t\t\tlogger.info(f'Correction of typos initiated done with {len(corrected_words)} corrected words.')\n",
    "\t\t\t\tcorrected_text = ' '.join(corrected_words)\n",
    "\n",
    "\t\t\t\treturn corrected_text\n",
    "\t\t\t\n",
    "\t\t\t# Для русского языка используем Яндекс.Спеллер\n",
    "\t\t\telif self.language == 'russian':\n",
    "\t\t\t\t\timport requests\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\turl = \"https://speller.yandex.net/services/spellservice.json/checkText\"\n",
    "\t\t\t\t\tparams = {\n",
    "\t\t\t\t\t\t\t'text': self.text,\n",
    "\t\t\t\t\t\t\t'lang': 'ru',\n",
    "\t\t\t\t\t\t\t'format': 'plain'\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tresponse = requests.get(url, params=params)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif response.status_code == 200:\n",
    "\t\t\t\t\t\t\tcorrections = response.json()\n",
    "\t\t\t\t\t\t\tresult_text = self.text\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# Применяем исправления в обратном порядке (с конца текста)\n",
    "\t\t\t\t\t\t\tfor item in reversed(corrections):\n",
    "\t\t\t\t\t\t\t\t\tpos = item['pos']\n",
    "\t\t\t\t\t\t\t\t\tlength = item['len']\n",
    "\t\t\t\t\t\t\t\t\tsuggestions = item['s']\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\tif suggestions:\n",
    "\t\t\t\t\t\t\t\t\t\t\t# Заменяем ошибочное слово на первое предложенное исправление\n",
    "\t\t\t\t\t\t\t\t\t\t\tresult_text = result_text[:pos] + suggestions[0] + result_text[pos + length:]\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tlogger.info(f'Corrected {len(corrections)} typos using Yandex.Speller')\n",
    "\t\t\t\t\t\t\treturn result_text\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlogger.error(f'Yandex.Speller API returned status code {response.status_code}')\n",
    "\t\t\t\t\t\treturn self.text\n",
    "\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.error(f'Typos correction error: {str(e)}')\n",
    "\t\t\treturn self.text\n",
    "\t\n",
    "\tdef __numbers_to_words(self) -> str: \n",
    "\t\t\"\"\"\n",
    "    Заменяет числа в тексте на их текстовые эквиваленты.\n",
    "    \n",
    "    Например:\n",
    "    - \"10 книг\" -> \"десять книг\"\n",
    "    - \"2 students\" -> \"two students\"\n",
    "    \n",
    "    :return: текст с числами, преобразованными в слова\n",
    "    \"\"\"\n",
    "\t\t\n",
    "\t\tlogger.info('Converting numbers to words...')\n",
    "\t\ttry: \n",
    "\n",
    "\t\t\tnum_lang = self.__lang_to_simple_form()\n",
    "\n",
    "\t\t\tdef replace_number(match): \n",
    "\t\t\t\t'''\n",
    "\t\t\t\tВызывается, когда находится соответствие маске '\\b\\d+[,.]?\\d*\\b'\n",
    "\n",
    "\t\t\t\t:param match: это часть текста, которая маске подошла.\n",
    "\t\t\t\t'''\n",
    "\t\t\t\ttry: \n",
    "\t\t\t\t\t# .group(0) - возвращает весь текст, а не только его какую-то группу. \n",
    "\t\t\t\t\tnumber = match.group(0)\n",
    "\n",
    "\t\t\t\t\tif '.' in number or ',' in number:\n",
    "\t\t\t\t\t\tnumber = number.replace(',', '.')\n",
    "\t\t\t\t\t\treturn num2words(float(number), lang=num_lang)\n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\treturn num2words(int(number), lang=num_lang)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\texcept ValueError: \n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\treturn match.group(0)\n",
    "\t\t\t\t\t\n",
    "\t\t\ttext_with_words = re.sub(r'\\b\\d+[,.]?\\d*\\b', replace_number, self.text)\n",
    "\n",
    "\t\t\tlogger.info('Numbers have been converted to words.')\n",
    "\t\t\treturn text_with_words \n",
    "\t\t\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.error(f'Error during number to word conversion: {str(e)}')\n",
    "\t\t\treturn self.text\n",
    "\n",
    "\tdef __delete_punctuation_marks(self) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tУдаляет знаки препинания из предложения.\n",
    "\n",
    "\t\tРежимы (delete_punctuation_mode): \n",
    "\t\t1. 're' — c помощью регулярных выражений. \n",
    "\t\t2. 'string' — с помощью библиотеки string.  \n",
    "\n",
    "\t\t:return: текст без знаков препинания. \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Punctuation are being deleting...')\n",
    "\n",
    "\t\tmatch self.delete_punctuation_mode: \n",
    "\t\t\tcase 're':\n",
    "\n",
    "\t\t\t\tlogger.info('Punctuation has been deleted.')\n",
    "\n",
    "\t\t\t\treturn re.sub(r'[^\\w\\s]', '', self.text)\n",
    "\t\t\t\n",
    "\t\t\tcase 'string':\n",
    "\t\t\t\t# Параметры: \n",
    "\t\t\t\t# Первый аргумент '' символы для замены \n",
    "\t\t\t\t# Второй аргумент '' символы на которые заменяем \n",
    "\t\t\t\t# Третий аргумент string.punctuation символы, которые нужно удалить \n",
    "\t\t\t\t# string.punctuation = \"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\" \n",
    "\t\t\t\ttranslator = str.maketrans('', '', ''.join(c for c in string.punctuation if c != ' '))\n",
    "\n",
    "\t\t\t\tlogger.info('Punctuation has been deleted.')\n",
    "\n",
    "\t\t\t\t# translate() применяет таблицу преобразования к строке \n",
    "\t\t\t\treturn self.text.translate(translator)\n",
    "\n",
    "\tdef __delete_stop_words(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tУдаляет стоп-слова. \n",
    "\n",
    "\t\tСтоп-слова — распространенные слова в языке, которые не несут значимой смысловой нагрузки \n",
    "\t\tпри анализе текста. \n",
    "\n",
    "\t\tЗачем их удалять? \n",
    "\t\t1. Текст становится меньше. \n",
    "\t\t2. Фокус на важных словах. \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Stop words are being removed...')\n",
    "\n",
    "\t\tstopwords.words(self.language)\n",
    "\n",
    "\t\tstop_words = set(stopwords.words(self.language))\n",
    "\t\t\n",
    "\t\twords = self.text.split()\n",
    "\n",
    "\t\tfiltered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "\t\tlogger.info('Stop words have been removed.')\n",
    "\t\t\n",
    "\t\treturn ' '.join(filtered_words)\n",
    "\t\t\n",
    "\tdef __stemming(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tВыполняет стемминг слов в тексте.\n",
    "\t\t\n",
    "\t\tСтемминг — процесс нахождения основы слова (стеммы) путем \n",
    "\t\tотбрасывания окончаний и суффиксов. Например, для слов \"рыбак\", \n",
    "\t\t\"рыбачить\", \"рыбный\" стеммой будет \"рыб\".\n",
    "\t\t\n",
    "\t\t:return: текст со словами, приведенными к их основам. \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Stemming is being performed...')\n",
    "\t\t\n",
    "\t\ttry: \n",
    "\t\t\t\n",
    "\t\t\twords = self.text.split() \n",
    "\t\t\tstemmed_words = [] \n",
    "\n",
    "\t\t\tmatch self.language: \n",
    "\t\t\t\tcase 'russian': \n",
    "\t\t\t\t\tstemmer = SnowballStemmer(\"russian\")\n",
    "\t\t\t\tcase 'english': \n",
    "\t\t\t\t\tstemmer = PorterStemmer()\n",
    "\t\t\t\tcase _: \n",
    "\t\t\t\t\tlogger.error(f'Stemming not supported for language: {self.language}')\n",
    "\t\t\t\t\treturn self.text\n",
    "\n",
    "\t\t\tfor word in words:\n",
    "\t\t\t\tstemmed_words.append(stemmer.stem(word))\n",
    "\t\t\t\n",
    "\t\t\tlogger.info('Stemming has been performed.')\n",
    "\t\t\treturn ' '.join(stemmed_words)\n",
    "\t\t\t\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.error(f'Error during stemming: {str(e)}')\n",
    "\t\t\treturn self.text\n",
    "\t\t\n",
    "\tdef __lemming(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tВыполняет лемматизацию слов в тексте.\n",
    "\t\t\t\n",
    "\t\tЛемматизация — процесс приведения слова к его словарной форме (лемме).\n",
    "\t\tНапример, для слов \"бегущий\", \"бегает\", \"бежит\" леммой будет \"бежать\".\n",
    "\t\t\t\n",
    "\t\t:return: текст со словами в словарной форме\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Lemmatizaion is being performed...')\n",
    "\n",
    "\t\ttry: \n",
    "\t\t\t\n",
    "\t\t\twords = self.text.split() \n",
    "\t\t\tlemmatized_words = []\n",
    "\n",
    "\t\t\tmatch self.language: \n",
    "\t\t\t\tcase 'russian': \n",
    "\t\t\t\t\t\tlemmatizer = Mystem()\n",
    "\n",
    "\t\t\t\t\t\tfor word in words: \n",
    "\t\t\t\t\t\t\ttry: \n",
    "\t\t\t\t\t\t\t\t# .parse(word) — библиотека pymorphy2 анализирует слово и \n",
    "\t\t\t\t\t\t\t\t# возвращает список всех возможных морфологических разборов этого слова.\n",
    "\t\t\t\t\t\t\t\t# мы берем [0] — наиболее вероятный\n",
    "\t\t\t\t\t\t\t\t#\n",
    "\t\t\t\t\t\t\t\t# После чего делаем нормальную форму (лемму) .normal_form\n",
    "\t\t\t\t\t\t\t\t# Для существительных - именительный падеж, единственное число\n",
    "\t\t\t\t\t\t\t\t# Для глаголов - инфинитив\n",
    "\t\t\t\t\t\t\t\t# Для прилагательных - мужской род, единственное число, именительный падеж\n",
    "\t\t\t\t\t\t\t\tlemma = ''.join(lemmatizer.lemmatize(word)).strip()\n",
    "\t\t\t\t\t\t\t\tlemmatized_words.append(lemma)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\tlogger.warning(f\"Failed to lemmatize word '{word}': {e}\")\n",
    "\t\t\t\t\t\t\t\tlemmatized_words.append(word)  # Сохраняем исходное слово\n",
    "\n",
    "\t\t\t\tcase 'english':\n",
    "\n",
    "\t\t\t\t\tlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\t\t\t\t\t# Разобраться, что делает эта функция \n",
    "\t\t\t\t\tdef get_wordnet_pos(word):\n",
    "\t\t\t\t\t\t\"\"\"Отображает тег части речи NLTK на тег WordNet\"\"\"\n",
    "\t\t\t\t\t\ttag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\t\t\t\t\t\ttag_dict = {\n",
    "\t\t\t\t\t\t\t\t'J': wordnet.ADJ,\n",
    "\t\t\t\t\t\t\t\t'N': wordnet.NOUN,\n",
    "\t\t\t\t\t\t\t\t'V': wordnet.VERB,\n",
    "\t\t\t\t\t\t\t\t'R': wordnet.ADV\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\treturn tag_dict.get(tag, wordnet.NOUN)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tfor word in words: \n",
    "\t\t\t\t\t\tlemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "\t\t\t\t\t\tlemmatized_words.append(lemma)\n",
    "\n",
    "\t\t\t\tcase _: \n",
    "\t\t\t\t\tlogger.error(f'Lemmatization not supported for language: {self.language}')\n",
    "\t\t\t\t\treturn self.text\n",
    "\t\t\t\n",
    "\t\t\tlogger.info('Lemmatization has been performed.')\n",
    "\t\t\treturn ' '.join(lemmatized_words)\n",
    "\t\t\t\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.error(f'Error during lemmatizaion: {str(e)}')\n",
    "\t\t\treturn self.text\n",
    "\n",
    "\tdef __text_normalization(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tВыбирает метод нормализации текста. \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmatch self.normalization_method: \n",
    "\t\t\tcase 'lemma': \n",
    "\t\t\t\treturn self.__lemming()\n",
    "\t\t\tcase 'stem': \n",
    "\t\t\t\treturn self.__stemming()\n",
    "\t\t\tcase _: \n",
    "\t\t\t\tlogger.warning('There is no such thing as a text normalization method')\n",
    "\t\t\t\tlogger.info('Returned text without normalization')\n",
    "\t\t\t\treturn self.text\n",
    "\n",
    "\tdef process_text(self) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tОбрабатывает текст, применяя все методы предобработки.\n",
    "\t\t\n",
    "\t\t:return: обработанный текст по всем функциям.\n",
    "\t\t\"\"\" \n",
    "\n",
    "\t\tfor func in [self.__to_lower_case, self.__numbers_to_words, self.__delete_punctuation_marks, self.__correct_spell, self.__delete_stop_words, self.__text_normalization]: \n",
    "\t\t\t\tself.text = func()\n",
    "\t\t\n",
    "\t\tlogger.info('Text preprocessing is done!')\n",
    "\n",
    "\t\treturn self.text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords: \n",
    "\t\"\"\"\n",
    "\tПреобразует текст в мешок слов. \n",
    "\n",
    "\tМешок слов (Bag of Words) - это представление текста, которое описывает \n",
    "  частоту появления слов в документе. Это простая модель, игнорирующая \n",
    "  порядок слов, но сохраняющая их количество.  \n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, preprocessed_text): \n",
    "\t\t\"\"\"\n",
    "\t\tИнициализация класса. \n",
    "\n",
    "\t\t:param preprocessed_text: предобработанный текст \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('BagOfWords class is initialized')\n",
    "\n",
    "\t\tif isinstance(preprocessed_text, dict):\n",
    "\t\t\t\tlogger.warning('Input is already a dictionary, not a text string')\n",
    "\t\t\t\tself.preprocessed_text = ' '.join(preprocessed_text.keys())\n",
    "\t\telse:\n",
    "\t\t\t\tself.preprocessed_text = preprocessed_text\n",
    "\t\n",
    "\tdef create_bag(self): \n",
    "\t\t\"\"\"\n",
    "    Создает мешок слов с помощью sklearn.\n",
    "    \n",
    "    :return: словарь, где ключи - слова, значения - частота их появления\n",
    "    \"\"\"\n",
    "\t\t\n",
    "\t\tlogger.info('Creating bag of words using sklearn...')\n",
    "\t\t\n",
    "\t\ttry: \n",
    "\t\t\tvectorizer = CountVectorizer()\n",
    "\n",
    "\t\t\t# fit() — обучает векторизатор \n",
    "\t\t\t# transform() — преобразует текст в числовое представление\n",
    "\t\t\t# преобразует текст в матрицу: \n",
    "\t\t\t# 1. строки соответствуют документа (в моем случае один)\n",
    "\t\t\t# 2. столбцы соответствуют уникальным словам из словаря \n",
    "\t\t\t# 3. значения в ячейках — частота появления слов в соответствующем документе \n",
    "\t\t\t#\n",
    "\t\t\t# X — объект scipy.sparse.csr_matrix (разреженная матрица)\n",
    "\t\t\t# CSR — compressed sparse row \n",
    "\t\t\t# такая матрица используется для эффективного хранения данных, где большинство элементов \n",
    "\t\t\t# равны нулю. \n",
    "\t\t\t#\n",
    "\t\t\t# Пример матрицы: \n",
    "\t\t\t# (0, 1)\t1\n",
    "\t\t\t# (0, 3)\t2\n",
    "\t\t\t# (0, 4)\t1\n",
    "\t\t\t# (0, 7)\t1\n",
    "\t\t\t# (0, 10)\t1\n",
    "\t\t\t#\n",
    "\t\t\t# (строка, столбец) сколько раз встречается \n",
    "\t\t\tX = vectorizer.fit_transform([self.preprocessed_text])\n",
    "\t\t\twords = vectorizer.get_feature_names_out()\n",
    "\n",
    "\t\t\tprint(words)\n",
    "\n",
    "\t\t\tword_freq = {} \n",
    "\t\t\tfor i, word in enumerate(words): \n",
    "\t\t\t\tcount = X.toarray()[0][i]\n",
    "\t\t\t\tif count > 0: \n",
    "\t\t\t\t\tword_freq[word] = count \n",
    "\n",
    "\t\t\tlogger.info(f'Bag of words created with {len(word_freq)} unique words using sklearn')\n",
    "\t\t\treturn word_freq \n",
    "\t\t\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.info(f'Bag of words created with {len(word_freq)} unique words using sklearn')\n",
    "\t\t\treturn {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrams: \n",
    "\t\"\"\"\n",
    "\tПреобразует текст в N-граммы - последовательности из N слов.\n",
    "\t\n",
    "\tN-грамма - это последовательность из N элементов (в данном случае, слов)\n",
    "\tидущих друг за другом в тексте.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, preprocessed_text):\n",
    "\t\t\"\"\"\n",
    "\t\tИнициализация класса.\n",
    "\t\t\n",
    "\t\t:param preprocessed_text: предобработанный текст\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('NGrams class is initialized')\n",
    "\n",
    "\t\tself.preprocessed_text = preprocessed_text\n",
    "\n",
    "\tdef create_ngrams(self, n=2):\n",
    "\t\t\"\"\"\n",
    "\t\tСоздаёт N-граммы из предобработанного текста. \n",
    "\n",
    "\t\t:param n: размер N-граммы (количество слов в одной N-грамме)\n",
    "    :return: словарь, где ключи - N-граммы, значения - частота их появления\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info(f'Creating {n}-grams...')\n",
    "\n",
    "\t\ttry: \n",
    "\t\t\twords = self.preprocessed_text.split() \n",
    "\n",
    "\t\t\t# Если текст короче, чем размер N-граммы, возвращаем пустой словарь\n",
    "\t\t\tif len(words) < n:\n",
    "\t\t\t\t\tlogger.warning(f'Text is too short to create {n}-grams')\n",
    "\t\t\t\t\treturn {}\n",
    "      \n",
    "\t\t\tngrams = []\n",
    "\t\t\tfor i in range(len(words) - n + 1): \n",
    "\t\t\t\tngram = ' '.join(words[i:i+n])\n",
    "\t\t\t\tngrams.append(ngram)\n",
    "\n",
    "\t\t\tngram_freq = {} \n",
    "\t\t\tfor ngram in ngrams: \n",
    "\t\t\t\tif ngram in ngram_freq: \n",
    "\t\t\t\t\tngram_freq[ngram] += 1 \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tngram_freq[ngram] = 1 \n",
    "\t\t\t\n",
    "\t\t\tlogger.info(f'{n}-grams created with {len(ngram_freq)} unique {n}-grams')\n",
    "\t\t\treturn ngram_freq \n",
    "\t\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.error(f'Error during {n}-grams creation: {str(e)}')\n",
    "\t\t\treturn {} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы с txt \n",
    "\n",
    "def read_txt(file_path): \n",
    "\t\"\"\"\n",
    "\tСчитывает текст из файла. \n",
    "\n",
    "\t:param file_path: путь к файлу \n",
    "\t:return: содержимое файла в виде строки \n",
    "\t\"\"\"\n",
    "\n",
    "\tlogger.info(f'Reading text from file: {file_path}')\n",
    "\n",
    "\ttry: \n",
    "\t\twith open(file_path, 'r', encoding='utf-8') as file: \n",
    "\t\t\ttext = file.read()\n",
    "\n",
    "\t\tlogger.info(f'Successfully read {len(text)} chars from file.')\n",
    "\n",
    "\t\treturn text\n",
    "\t\n",
    "\texcept Exception as e: \n",
    "\t\tlogger.critical(f'Didn`t get a chance to read the text: {str(e)}')\n",
    "\t\treturn \"\"\n",
    "\t\n",
    "\t\n",
    "def text_to_txt(file_path, text): \n",
    "\t\"\"\"\n",
    "\tЗаписывает текст в txt. \n",
    "\n",
    "\t:param file_path: путь к файлу, куда записать текст\n",
    "\t:param text: сам обработанный текст\n",
    "\t\"\"\"\n",
    "\n",
    "\tlogger.info(f'Transmitted preprocessed text with {len(text)} chars.')\n",
    "\n",
    "\ttry: \n",
    "\t\twith open(file_path, 'w', encoding='utf-8') as file: \n",
    "\t\t\t\n",
    "\t\t\t# Этот блок кода сделан специально, чтобы текст можно было читать не одним\n",
    "\t\t\t# сплошным предложением\n",
    "\t\t\ttext = text.split()\n",
    "\t\t\tline = ''\n",
    "\t\t\tresult = []\n",
    "\t\t\tfor i, word in enumerate(text): \n",
    "\t\t\t\tif i % 15 == 0 and i != 0: \n",
    "\t\t\t\t\tline.rstrip()\n",
    "\t\t\t\t\tline += '\\n'\n",
    "\t\t\t\t\tresult.append(line)\n",
    "\t\t\t\t\tline = f'{word} '\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tline += f'{word} '\n",
    "\n",
    "\t\t\tif line: result.append(line)\n",
    "\n",
    "\t\t\tresult = ''.join(result)\n",
    "\n",
    "\t\t\tfile.write(result)\n",
    "\n",
    "\t\t\tlogger.info(f'Text successfully was written to {file_path}')\n",
    "\n",
    "\texcept Exception as e: \n",
    "\t\tlogger.critical(f'Error to write the text: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:57:24,335 - INFO - Reading text from file: D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/test_text.txt\n",
      "2025-03-26 21:57:24,336 - INFO - Successfully read 190 chars from file.\n",
      "2025-03-26 21:57:24,337 - INFO - Text is initializing...\n",
      "2025-03-26 21:57:24,340 - INFO - Detected language: russian\n",
      "2025-03-26 21:57:24,341 - INFO - Text is initialized with parameters: delete_punctuation_mode=string, text_normalization_method=lemma\n",
      "2025-03-26 21:57:24,343 - INFO - Text has been converted to lower case.\n",
      "2025-03-26 21:57:24,344 - INFO - Converting numbers to words...\n",
      "2025-03-26 21:57:24,346 - INFO - Numbers have been converted to words.\n",
      "2025-03-26 21:57:24,347 - INFO - Punctuation are being deleting...\n",
      "2025-03-26 21:57:24,349 - INFO - Punctuation has been deleted.\n",
      "2025-03-26 21:57:24,350 - INFO - Correction of typos initiated...\n",
      "2025-03-26 21:57:25,382 - INFO - Corrected 0 typos using Yandex.Speller\n",
      "2025-03-26 21:57:25,382 - INFO - Text preprocessing is done!\n"
     ]
    }
   ],
   "source": [
    "russian_input_file_path = 'D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/test_text.txt'\n",
    "text = read_txt(russian_input_file_path)\n",
    "\n",
    "output = TextPreprocessing(\n",
    "\ttext, \n",
    "\tdelete_punctuation_mode='string', \n",
    "\ttext_normalization_method='lemma').process_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:57:25,390 - INFO - Transmitted preprocessed text with 254 chars.\n",
      "2025-03-26 21:57:25,391 - INFO - Text successfully was written to D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/output_russian.txt\n"
     ]
    }
   ],
   "source": [
    "# Записываем текст в файл \n",
    "\n",
    "russian_output_file_path = 'D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/output_russian.txt'\n",
    "text_to_txt(russian_output_file_path, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:57:25,399 - INFO - BagOfWords class is initialized\n",
      "2025-03-26 21:57:25,400 - INFO - Creating bag of words using sklearn...\n",
      "2025-03-26 21:57:25,404 - INFO - Bag of words created with 32 unique words using sklearn\n",
      "2025-03-26 21:57:25,405 - INFO - NGrams class is initialized\n",
      "2025-03-26 21:57:25,405 - INFO - Creating 2-grams...\n",
      "2025-03-26 21:57:25,406 - INFO - 2-grams created with 36 unique 2-grams\n",
      "2025-03-26 21:57:25,407 - INFO - NGrams class is initialized\n",
      "2025-03-26 21:57:25,408 - INFO - Creating 4-grams...\n",
      "2025-03-26 21:57:25,408 - INFO - 4-grams created with 34 unique 4-grams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бизнеса' 'будет' 'вторая' 'дeла' 'два' 'двадцать' 'жизнь' 'интересных'\n",
      " 'как' 'надеюсь' 'одиннадцать' 'пoвседневную' 'пoговорить' 'первая'\n",
      " 'полезно' 'привет' 'пятьсот' 'секрета' 'семь' 'сотых' 'способов' 'сто'\n",
      " 'темах' 'три' 'тридцать' 'тысячных' 'улучшить' 'успешного' 'хoчу' 'целых'\n",
      " 'четыре' 'это']\n",
      "{'бизнеса': 1, 'будет': 1, 'вторая': 1, 'дeла': 1, 'два': 1, 'двадцать': 1, 'жизнь': 1, 'интересных': 1, 'как': 1, 'надеюсь': 1, 'одиннадцать': 1, 'пoвседневную': 1, 'пoговорить': 1, 'первая': 1, 'полезно': 1, 'привет': 1, 'пятьсот': 1, 'секрета': 1, 'семь': 1, 'сотых': 1, 'способов': 1, 'сто': 1, 'темах': 1, 'три': 2, 'тридцать': 1, 'тысячных': 1, 'улучшить': 1, 'успешного': 1, 'хoчу': 1, 'целых': 2, 'четыре': 1, 'это': 2}\n",
      "{'привет как': 1, 'как дeла': 1, 'дeла я': 1, 'я хoчу': 1, 'хoчу пoговорить': 1, 'пoговорить о': 1, 'о два': 1, 'два интересных': 1, 'интересных темах': 1, 'темах первая': 1, 'первая это': 1, 'это семь': 1, 'семь способов': 1, 'способов улучшить': 1, 'улучшить пoвседневную': 1, 'пoвседневную жизнь': 1, 'жизнь вторая': 1, 'вторая три': 1, 'три секрета': 1, 'секрета успешного': 1, 'успешного бизнеса': 1, 'бизнеса надеюсь': 1, 'надеюсь это': 1, 'это будет': 1, 'будет полезно': 1, 'полезно сто': 1, 'сто целых': 1, 'целых пятьсот': 1, 'пятьсот тридцать': 1, 'тридцать четыре': 1, 'четыре тысячных': 1, 'тысячных одиннадцать': 1, 'одиннадцать целых': 1, 'целых двадцать': 1, 'двадцать три': 1, 'три сотых': 1}\n",
      "{'привет как дeла я': 1, 'как дeла я хoчу': 1, 'дeла я хoчу пoговорить': 1, 'я хoчу пoговорить о': 1, 'хoчу пoговорить о два': 1, 'пoговорить о два интересных': 1, 'о два интересных темах': 1, 'два интересных темах первая': 1, 'интересных темах первая это': 1, 'темах первая это семь': 1, 'первая это семь способов': 1, 'это семь способов улучшить': 1, 'семь способов улучшить пoвседневную': 1, 'способов улучшить пoвседневную жизнь': 1, 'улучшить пoвседневную жизнь вторая': 1, 'пoвседневную жизнь вторая три': 1, 'жизнь вторая три секрета': 1, 'вторая три секрета успешного': 1, 'три секрета успешного бизнеса': 1, 'секрета успешного бизнеса надеюсь': 1, 'успешного бизнеса надеюсь это': 1, 'бизнеса надеюсь это будет': 1, 'надеюсь это будет полезно': 1, 'это будет полезно сто': 1, 'будет полезно сто целых': 1, 'полезно сто целых пятьсот': 1, 'сто целых пятьсот тридцать': 1, 'целых пятьсот тридцать четыре': 1, 'пятьсот тридцать четыре тысячных': 1, 'тридцать четыре тысячных одиннадцать': 1, 'четыре тысячных одиннадцать целых': 1, 'тысячных одиннадцать целых двадцать': 1, 'одиннадцать целых двадцать три': 1, 'целых двадцать три сотых': 1}\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = BagOfWords(output).create_bag()\n",
    "bigrams = NGrams(output).create_ngrams(n=2)\n",
    "ngrams = NGrams(output).create_ngrams(n=4)\n",
    "\n",
    "\n",
    "print(bag_of_words)\n",
    "print(bigrams)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:57:25,418 - INFO - Reading text from file: D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/english_text.txt\n",
      "2025-03-26 21:57:25,419 - INFO - Successfully read 850 chars from file.\n",
      "2025-03-26 21:57:25,419 - INFO - Text is initializing...\n",
      "2025-03-26 21:57:25,424 - INFO - Detected language: english\n",
      "2025-03-26 21:57:25,425 - INFO - Text is initialized with parameters: delete_punctuation_mode=string, text_normalization_method=lemma\n",
      "2025-03-26 21:57:25,425 - INFO - Text has been converted to lower case.\n",
      "2025-03-26 21:57:25,426 - INFO - Converting numbers to words...\n",
      "2025-03-26 21:57:25,427 - INFO - Numbers have been converted to words.\n",
      "2025-03-26 21:57:25,427 - INFO - Punctuation are being deleting...\n",
      "2025-03-26 21:57:25,428 - INFO - Punctuation has been deleted.\n",
      "2025-03-26 21:57:25,428 - INFO - Correction of typos initiated...\n",
      "2025-03-26 21:57:26,014 - INFO - Correction of typos initiated done with 134 corrected words.\n",
      "2025-03-26 21:57:26,018 - INFO - Text preprocessing is done!\n"
     ]
    }
   ],
   "source": [
    "english_input_file_path = 'D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/english_text.txt'\n",
    "text = read_txt(english_input_file_path)\n",
    "\n",
    "eng_output = TextPreprocessing(\n",
    "\ttext, \n",
    "\ttext_normalization_method='lemma', \n",
    "\tdelete_punctuation_mode='string').process_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:57:26,024 - INFO - Transmitted preprocessed text with 841 chars.\n",
      "2025-03-26 21:57:26,026 - INFO - Text successfully was written to D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/english_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Записываем текст в файл \n",
    "\n",
    "english_output_file_path = 'D:/Projects/SvetlanaDmitrievna/lab3_ii/texts/output_english.txt'\n",
    "text_to_txt(english_input_file_path, eng_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:57:26,034 - INFO - BagOfWords class is initialized\n",
      "2025-03-26 21:57:26,035 - INFO - Creating bag of words using sklearn...\n",
      "2025-03-26 21:57:26,037 - INFO - Bag of words created with 119 unique words using sklearn\n",
      "2025-03-26 21:57:26,038 - INFO - NGrams class is initialized\n",
      "2025-03-26 21:57:26,038 - INFO - Creating 2-grams...\n",
      "2025-03-26 21:57:26,039 - INFO - 2-grams created with 133 unique 2-grams\n",
      "2025-03-26 21:57:26,039 - INFO - NGrams class is initialized\n",
      "2025-03-26 21:57:26,040 - INFO - Creating 4-grams...\n",
      "2025-03-26 21:57:26,041 - INFO - 4-grams created with 131 unique 4-grams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able' 'although' 'arch' 'arm' 'armourlike' 'back' 'bed' 'bedding'\n",
      " 'belly' 'bit' 'boa' 'brown' 'collection' 'compare' 'could' 'cover' 'cut'\n",
      " 'divide' 'domed' 'dream' 'drop' 'dull' 'familiar' 'feel' 'fit' 'forget'\n",
      " 'found' 'four' 'frame' 'fur' 'get' 'gild' 'gregory' 'happen' 'hardly'\n",
      " 'hat' 'head' 'heard' 'heavy' 'helplessly' 'hit' 'horrible' 'house'\n",
      " 'however' 'human' 'hung' 'illustrate' 'lady' 'lay' 'leg' 'lift' 'little'\n",
      " 'longer' 'look' 'low' 'magazine' 'make' 'many' 'moment' 'morning' 'muff'\n",
      " 'nice' 'nonsense' 'one' 'pane' 'peacefully' 'picture' 'pitifully'\n",
      " 'position' 'present' 'proper' 'quite' 'rain' 'raise' 'ready' 'recently'\n",
      " 'rest' 'right' 'room' 'sad' 'salesman' 'salsa' 'sample' 'sat' 'section'\n",
      " 'see' 'seem' 'show' 'size' 'sleep' 'slide' 'slightly' 'small' 'something'\n",
      " 'spread' 'state' 'stiff' 'table' 'textile' 'thin' 'thought' 'towards'\n",
      " 'transform' 'travel' 'troubled' 'turn' 'unable' 'upright' 'use' 'vermin'\n",
      " 'viewer' 'wall' 'want' 'wave' 'weather' 'whats' 'whole' 'window' 'woke']\n",
      "{'able': 1, 'although': 1, 'arch': 1, 'arm': 1, 'armourlike': 1, 'back': 1, 'bed': 1, 'bedding': 1, 'belly': 1, 'bit': 1, 'boa': 1, 'brown': 1, 'collection': 1, 'compare': 1, 'could': 2, 'cover': 2, 'cut': 1, 'divide': 1, 'domed': 1, 'dream': 2, 'drop': 1, 'dull': 1, 'familiar': 1, 'feel': 1, 'fit': 1, 'forget': 1, 'found': 1, 'four': 1, 'frame': 1, 'fur': 3, 'get': 1, 'gild': 1, 'gregory': 2, 'happen': 1, 'hardly': 1, 'hat': 1, 'head': 1, 'heard': 1, 'heavy': 1, 'helplessly': 1, 'hit': 1, 'horrible': 1, 'house': 1, 'however': 1, 'human': 1, 'hung': 1, 'illustrate': 1, 'lady': 1, 'lay': 3, 'leg': 1, 'lift': 1, 'little': 3, 'longer': 1, 'look': 2, 'low': 1, 'magazine': 1, 'make': 1, 'many': 1, 'moment': 1, 'morning': 1, 'muff': 1, 'nice': 1, 'nonsense': 1, 'one': 1, 'pane': 1, 'peacefully': 1, 'picture': 1, 'pitifully': 1, 'position': 1, 'present': 1, 'proper': 1, 'quite': 1, 'rain': 1, 'raise': 1, 'ready': 1, 'recently': 1, 'rest': 1, 'right': 1, 'room': 2, 'sad': 1, 'salesman': 1, 'salsa': 2, 'sample': 1, 'sat': 1, 'section': 1, 'see': 1, 'seem': 1, 'show': 1, 'size': 1, 'sleep': 2, 'slide': 1, 'slightly': 1, 'small': 1, 'something': 1, 'spread': 1, 'state': 1, 'stiff': 1, 'table': 1, 'textile': 1, 'thin': 1, 'thought': 2, 'towards': 1, 'transform': 1, 'travel': 1, 'troubled': 1, 'turn': 1, 'unable': 1, 'upright': 1, 'use': 1, 'vermin': 1, 'viewer': 1, 'wall': 1, 'want': 1, 'wave': 1, 'weather': 1, 'whats': 1, 'whole': 1, 'window': 1, 'woke': 1}\n",
      "{'one morning': 1, 'morning gregory': 1, 'gregory salsa': 1, 'salsa woke': 1, 'woke troubled': 1, 'troubled dream': 1, 'dream found': 1, 'found transform': 1, 'transform bed': 1, 'bed horrible': 1, 'horrible vermin': 1, 'vermin lay': 1, 'lay armourlike': 1, 'armourlike back': 1, 'back lift': 1, 'lift head': 1, 'head little': 1, 'little could': 1, 'could see': 1, 'see brown': 1, 'brown belly': 1, 'belly slightly': 1, 'slightly domed': 1, 'domed divide': 1, 'divide arch': 1, 'arch stiff': 1, 'stiff section': 1, 'section bedding': 1, 'bedding hardly': 1, 'hardly able': 1, 'able cover': 1, 'cover seem': 1, 'seem ready': 1, 'ready slide': 1, 'slide moment': 1, 'moment many': 1, 'many leg': 1, 'leg pitifully': 1, 'pitifully thin': 1, 'thin compare': 1, 'compare size': 1, 'size rest': 1, 'rest wave': 1, 'wave helplessly': 1, 'helplessly look': 1, 'look whats': 1, 'whats happen': 1, 'happen thought': 1, 'thought want': 1, 'want dream': 1, 'dream room': 1, 'room proper': 1, 'proper human': 1, 'human room': 1, 'room although': 1, 'although little': 1, 'little small': 1, 'small lay': 1, 'lay peacefully': 1, 'peacefully four': 1, 'four familiar': 1, 'familiar wall': 1, 'wall collection': 1, 'collection textile': 1, 'textile sample': 1, 'sample lay': 1, 'lay spread': 1, 'spread table': 1, 'table salsa': 1, 'salsa travel': 1, 'travel salesman': 1, 'salesman hung': 1, 'hung picture': 1, 'picture recently': 1, 'recently cut': 1, 'cut illustrate': 1, 'illustrate magazine': 1, 'magazine house': 1, 'house nice': 1, 'nice gild': 1, 'gild frame': 1, 'frame show': 1, 'show lady': 1, 'lady fit': 1, 'fit fur': 1, 'fur hat': 1, 'hat fur': 1, 'fur boa': 1, 'boa sat': 1, 'sat upright': 1, 'upright raise': 1, 'raise heavy': 1, 'heavy fur': 1, 'fur muff': 1, 'muff cover': 1, 'cover whole': 1, 'whole low': 1, 'low arm': 1, 'arm towards': 1, 'towards viewer': 1, 'viewer gregory': 1, 'gregory turn': 1, 'turn look': 1, 'look window': 1, 'window dull': 1, 'dull weather': 1, 'weather drop': 1, 'drop rain': 1, 'rain could': 1, 'could heard': 1, 'heard hit': 1, 'hit pane': 1, 'pane make': 1, 'make feel': 1, 'feel quite': 1, 'quite sad': 1, 'sad sleep': 1, 'sleep little': 1, 'little bit': 1, 'bit longer': 1, 'longer forget': 1, 'forget nonsense': 1, 'nonsense thought': 1, 'thought something': 1, 'something unable': 1, 'unable use': 1, 'use sleep': 1, 'sleep right': 1, 'right present': 1, 'present state': 1, 'state get': 1, 'get position': 1, 'position however': 1}\n",
      "{'one morning gregory salsa': 1, 'morning gregory salsa woke': 1, 'gregory salsa woke troubled': 1, 'salsa woke troubled dream': 1, 'woke troubled dream found': 1, 'troubled dream found transform': 1, 'dream found transform bed': 1, 'found transform bed horrible': 1, 'transform bed horrible vermin': 1, 'bed horrible vermin lay': 1, 'horrible vermin lay armourlike': 1, 'vermin lay armourlike back': 1, 'lay armourlike back lift': 1, 'armourlike back lift head': 1, 'back lift head little': 1, 'lift head little could': 1, 'head little could see': 1, 'little could see brown': 1, 'could see brown belly': 1, 'see brown belly slightly': 1, 'brown belly slightly domed': 1, 'belly slightly domed divide': 1, 'slightly domed divide arch': 1, 'domed divide arch stiff': 1, 'divide arch stiff section': 1, 'arch stiff section bedding': 1, 'stiff section bedding hardly': 1, 'section bedding hardly able': 1, 'bedding hardly able cover': 1, 'hardly able cover seem': 1, 'able cover seem ready': 1, 'cover seem ready slide': 1, 'seem ready slide moment': 1, 'ready slide moment many': 1, 'slide moment many leg': 1, 'moment many leg pitifully': 1, 'many leg pitifully thin': 1, 'leg pitifully thin compare': 1, 'pitifully thin compare size': 1, 'thin compare size rest': 1, 'compare size rest wave': 1, 'size rest wave helplessly': 1, 'rest wave helplessly look': 1, 'wave helplessly look whats': 1, 'helplessly look whats happen': 1, 'look whats happen thought': 1, 'whats happen thought want': 1, 'happen thought want dream': 1, 'thought want dream room': 1, 'want dream room proper': 1, 'dream room proper human': 1, 'room proper human room': 1, 'proper human room although': 1, 'human room although little': 1, 'room although little small': 1, 'although little small lay': 1, 'little small lay peacefully': 1, 'small lay peacefully four': 1, 'lay peacefully four familiar': 1, 'peacefully four familiar wall': 1, 'four familiar wall collection': 1, 'familiar wall collection textile': 1, 'wall collection textile sample': 1, 'collection textile sample lay': 1, 'textile sample lay spread': 1, 'sample lay spread table': 1, 'lay spread table salsa': 1, 'spread table salsa travel': 1, 'table salsa travel salesman': 1, 'salsa travel salesman hung': 1, 'travel salesman hung picture': 1, 'salesman hung picture recently': 1, 'hung picture recently cut': 1, 'picture recently cut illustrate': 1, 'recently cut illustrate magazine': 1, 'cut illustrate magazine house': 1, 'illustrate magazine house nice': 1, 'magazine house nice gild': 1, 'house nice gild frame': 1, 'nice gild frame show': 1, 'gild frame show lady': 1, 'frame show lady fit': 1, 'show lady fit fur': 1, 'lady fit fur hat': 1, 'fit fur hat fur': 1, 'fur hat fur boa': 1, 'hat fur boa sat': 1, 'fur boa sat upright': 1, 'boa sat upright raise': 1, 'sat upright raise heavy': 1, 'upright raise heavy fur': 1, 'raise heavy fur muff': 1, 'heavy fur muff cover': 1, 'fur muff cover whole': 1, 'muff cover whole low': 1, 'cover whole low arm': 1, 'whole low arm towards': 1, 'low arm towards viewer': 1, 'arm towards viewer gregory': 1, 'towards viewer gregory turn': 1, 'viewer gregory turn look': 1, 'gregory turn look window': 1, 'turn look window dull': 1, 'look window dull weather': 1, 'window dull weather drop': 1, 'dull weather drop rain': 1, 'weather drop rain could': 1, 'drop rain could heard': 1, 'rain could heard hit': 1, 'could heard hit pane': 1, 'heard hit pane make': 1, 'hit pane make feel': 1, 'pane make feel quite': 1, 'make feel quite sad': 1, 'feel quite sad sleep': 1, 'quite sad sleep little': 1, 'sad sleep little bit': 1, 'sleep little bit longer': 1, 'little bit longer forget': 1, 'bit longer forget nonsense': 1, 'longer forget nonsense thought': 1, 'forget nonsense thought something': 1, 'nonsense thought something unable': 1, 'thought something unable use': 1, 'something unable use sleep': 1, 'unable use sleep right': 1, 'use sleep right present': 1, 'sleep right present state': 1, 'right present state get': 1, 'present state get position': 1, 'state get position however': 1}\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = BagOfWords(eng_output).create_bag()\n",
    "bigrams = NGrams(eng_output).create_ngrams(n=2)\n",
    "ngrams = NGrams(eng_output).create_ngrams(n=4)\n",
    "\n",
    "print(bag_of_words)\n",
    "print(bigrams)\n",
    "print(ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
