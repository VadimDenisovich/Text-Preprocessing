{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторная работа 2. Обработка текста.\n",
    "\n",
    "Задание:\n",
    "\n",
    "Написать программу на языке программирования Python, реализующую:\n",
    "1)\tПредварительную обработку текста, включающую:\n",
    "\t-\tПеревод текста в нижний регистр\n",
    "\t-\tУдаление знаков препинания\n",
    "\t-\tУдаление стоп-слов\n",
    "\t-\tУдаление лишних символов\n",
    "\t-\tСтемминг и лемматизацию\n",
    "2)\tПреобразование текста в мешок слов\n",
    "3)\tПреобразование текста в N-граммы, где N=2,4\n",
    "\n",
    "Замечание!\n",
    "Желательно провести предварительную обработку текста с использованием библиотеки NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer # Стеммер для русского языка\n",
    "from nltk.stem.porter import PorterStemmer # Стеммер для английского \n",
    "from nltk.stem import WordNetLemmatizer # Лемматайзер для английского\n",
    "from nltk.corpus import wordnet\n",
    "from pymystem3 import Mystem # Лемматайзер для русского языка \n",
    "import re\n",
    "import string\n",
    "from langdetect import detect\n",
    "import logging\n",
    "import os \n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\vadig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Конфигурационный блок \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = r'D:\\Projects\\SvetlanaDmitrievna\\lab3_ii\\logs'\n",
    "os.makedirs(log_dir, exist_ok=True) \n",
    "log_filename = os.path.join(log_dir, f'{datetime.now().strftime(\"%H %M %S %Y %m %d\")} nlp.log')\n",
    "\n",
    "# Удаляем логеры, которые уже запущены по случайности \n",
    "logger = logging.getLogger()\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "    \n",
    "# Настройки для логгера \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # Вывод в консоль \n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing: \n",
    "\t\"\"\"\n",
    "\tTODO: сделать документацию. \n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, text, delete_punctuation_mode='string', text_normalization_method='lemma'):\n",
    "\t\t\"\"\"\n",
    "\t\tИнициализация класса TextPreprocessing.\n",
    "\n",
    "\t\t:param text: исходный текст для предобработки\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Text is initializing...')\n",
    "\n",
    "\t\tself.text = text\n",
    "\t\tself.delete_punctuation_mode = delete_punctuation_mode\n",
    "\t\tself.language = self.__detect_language() \n",
    "\t\tself.normalization_method = text_normalization_method\n",
    "\n",
    "\t\tlogger.info(f'Text is initialized with parameters: delete_punctuation_mode={delete_punctuation_mode}, text_normalization_method={text_normalization_method}')\n",
    "\t\n",
    "\t# def __lang_to_simple(self) -> str:\n",
    "\t# \t\"\"\"\n",
    "\t# \tВыдаёт аббревиатуру языка.\n",
    "\t# \t\"\"\"\n",
    "\t# \tlang_mapping = {\n",
    "\t# \t\t'russian': 'ru',\n",
    "\t# \t\t'english': 'en'\n",
    "\t# \t}\n",
    "\n",
    "\t# \treturn lang_mapping[self.language]\n",
    " \n",
    "\tdef __detect_language(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tАвтоматическое определение языка с помощью библиотеки langdetect. \n",
    "\n",
    "\t\t:return: Возвращается формат языка в стиле 'russian', 'english'.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tdetected_lang = detect(self.text)\n",
    "\t\t\t\n",
    "\t\t\tlang_mapping = {\n",
    "\t\t\t\t'ru': 'russian',\n",
    "\t\t\t\t'en': 'english',\n",
    "\t\t\t\t# TODO: create more languages\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tlanguage = lang_mapping[detected_lang]\n",
    "\n",
    "\t\t\tlogger.info(f'Detected language: {language}')\n",
    "\t\t\t\n",
    "\t\t\treturn language\n",
    "\t\t\n",
    "\t\texcept:\n",
    "\t\t\tlogger.critical('Error with detecting langauge.')\n",
    "\n",
    "\tdef __to_lower_case(self) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tПриводит текст к нижнему регистру. \n",
    "\n",
    "\t\t:return: возвращаем измененный текст. \n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tlogger.info('Text has been converted to lower case.')\n",
    "\n",
    "\t\treturn self.text.lower()\n",
    "\n",
    "\tdef __delete_punctuation_marks(self) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tУдаляет знаки препинания из предложения.\n",
    "\n",
    "\t\tРежимы (delete_punctuation_mode): \n",
    "\t\t1. 're' — c помощью регулярных выражений. \n",
    "\t\t2. 'string' — с помощью библиотеки string.  \n",
    "\n",
    "\t\t:return: текст без знаков препинания. \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Punctuation are being deleting...')\n",
    "\n",
    "\t\tmatch self.delete_punctuation_mode: \n",
    "\t\t\tcase 're':\n",
    "\n",
    "\t\t\t\tlogger.info('Punctuation has been deleted.')\n",
    "\n",
    "\t\t\t\treturn re.sub(r'[^\\w\\s]', '', self.text)\n",
    "\t\t\t\n",
    "\t\t\tcase 'string':\n",
    "\t\t\t\t# Параметры: \n",
    "\t\t\t\t# Первый аргумент '' символы для замены \n",
    "\t\t\t\t# Второй аргумент '' символы на которые заменяем \n",
    "\t\t\t\t# Третий аргумент string.punctuation символы, которые нужно удалить \n",
    "\t\t\t\t# string.punctuation = \"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\" \n",
    "\t\t\t\ttranslator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\t\t\t\tlogger.info('Punctuation has been deleted.')\n",
    "\n",
    "\t\t\t\t# translate() применяет таблицу преобразования к строке \n",
    "\t\t\t\treturn self.text.translate(translator)\n",
    "\n",
    "\tdef __delete_stop_words(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tУдаляет стоп-слова. \n",
    "\n",
    "\t\tСтоп-слова — распространенные слова в языке, которые не несут значимой смысловой нагрузки \n",
    "\t\tпри анализе текста. \n",
    "\n",
    "\t\tЗачем их удалять? \n",
    "\t\t1. Текст становится меньше. \n",
    "\t\t2. Фокус на важных словах. \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Stop words are being removed...')\n",
    "\n",
    "\t\tstopwords.words(self.language)\n",
    "\n",
    "\t\tstop_words = set(stopwords.words(self.language))\n",
    "\t\t\n",
    "\t\twords = self.text.split()\n",
    "\n",
    "\t\tfiltered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "\t\tlogger.info('Stop words have been removed.')\n",
    "\t\t\n",
    "\t\treturn ' '.join(filtered_words)\n",
    "\t\t\n",
    "\tdef __stemming(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\tВыполняет стемминг слов в тексте.\n",
    "\t\n",
    "\tСтемминг — процесс нахождения основы слова (стеммы) путем \n",
    "\tотбрасывания окончаний и суффиксов. Например, для слов \"рыбак\", \n",
    "\t\"рыбачить\", \"рыбный\" стеммой будет \"рыб\".\n",
    "\t\n",
    "\t:return: текст со словами, приведенными к их основам. \n",
    "\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Stemming is being performed...')\n",
    "\t\t\n",
    "\t\ttry: \n",
    "\t\t\t\n",
    "\t\t\twords = self.text.split() \n",
    "\t\t\tstemmed_words = [] \n",
    "\n",
    "\t\t\tmatch self.language: \n",
    "\t\t\t\tcase 'russian': \n",
    "\t\t\t\t\tstemmer = SnowballStemmer(\"russian\")\n",
    "\t\t\t\tcase 'english': \n",
    "\t\t\t\t\tstemmer = PorterStemmer()\n",
    "\t\t\t\tcase _: \n",
    "\t\t\t\t\tlogger.error(f'Stemming not supported for language: {self.language}')\n",
    "\t\t\t\t\treturn self.text\n",
    "\n",
    "\t\t\tfor word in words:\n",
    "\t\t\t\tstemmed_words.append(stemmer.stem(word))\n",
    "\t\t\t\n",
    "\t\t\tlogger.info('Stemming has been performed.')\n",
    "\t\t\treturn ' '.join(stemmed_words)\n",
    "\t\t\t\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.error(f'Error during stemming: {str(e)}')\n",
    "\t\t\treturn self.text\n",
    "\t\t\n",
    "\tdef __lemming(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tВыполняет лемматизацию слов в тексте.\n",
    "\t\t\t\n",
    "\t\tЛемматизация — процесс приведения слова к его словарной форме (лемме).\n",
    "\t\tНапример, для слов \"бегущий\", \"бегает\", \"бежит\" леммой будет \"бежать\".\n",
    "\t\t\t\n",
    "\t\t:return: текст со словами в словарной форме\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tlogger.info('Lemmatizaion is being performed...')\n",
    "\n",
    "\t\ttry: \n",
    "\t\t\t\n",
    "\t\t\twords = self.text.split() \n",
    "\t\t\tlemmatized_words = []\n",
    "\n",
    "\t\t\tmatch self.language: \n",
    "\t\t\t\tcase 'russian': \n",
    "\t\t\t\t\t\tlemmatizer = Mystem()\n",
    "\n",
    "\t\t\t\t\t\tfor word in words: \n",
    "\t\t\t\t\t\t\ttry: \n",
    "\t\t\t\t\t\t\t\t# .parse(word) — библиотека pymorphy2 анализирует слово и \n",
    "\t\t\t\t\t\t\t\t# возвращает список всех возможных морфологических разборов этого слова.\n",
    "\t\t\t\t\t\t\t\t# мы берем [0] — наиболее вероятный\n",
    "\t\t\t\t\t\t\t\t#\n",
    "\t\t\t\t\t\t\t\t# После чего делаем нормальную форму (лемму) .normal_form\n",
    "\t\t\t\t\t\t\t\t# Для существительных - именительный падеж, единственное число\n",
    "\t\t\t\t\t\t\t\t# Для глаголов - инфинитив\n",
    "\t\t\t\t\t\t\t\t# Для прилагательных - мужской род, единственное число, именительный падеж\n",
    "\t\t\t\t\t\t\t\tlemma = ''.join(lemmatizer.lemmatize(word)).strip()\n",
    "\t\t\t\t\t\t\t\tlemmatized_words.append(lemma)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\tlogger.warning(f\"Failed to lemmatize word '{word}': {e}\")\n",
    "\t\t\t\t\t\t\t\tlemmatized_words.append(word)  # Сохраняем исходное слово\n",
    "\n",
    "\t\t\t\tcase 'english':\n",
    "\n",
    "\t\t\t\t\tlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\t\t\t\t\t# Разобраться, что делает эта функция \n",
    "\t\t\t\t\tdef get_wordnet_pos(word):\n",
    "\t\t\t\t\t\t\"\"\"Отображает тег части речи NLTK на тег WordNet\"\"\"\n",
    "\t\t\t\t\t\ttag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\t\t\t\t\t\ttag_dict = {\n",
    "\t\t\t\t\t\t\t\t'J': wordnet.ADJ,\n",
    "\t\t\t\t\t\t\t\t'N': wordnet.NOUN,\n",
    "\t\t\t\t\t\t\t\t'V': wordnet.VERB,\n",
    "\t\t\t\t\t\t\t\t'R': wordnet.ADV\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\treturn tag_dict.get(tag, wordnet.NOUN)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tfor word in words: \n",
    "\t\t\t\t\t\tlemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "\t\t\t\t\t\tlemmatized_words.append(lemma)\n",
    "\n",
    "\t\t\t\tcase _: \n",
    "\t\t\t\t\tlogger.error(f'Lemmatization not supported for language: {self.language}')\n",
    "\t\t\t\t\treturn self.text\n",
    "\t\t\t\n",
    "\t\t\tlogger.info('Lemmatization has been performed.')\n",
    "\t\t\treturn ' '.join(lemmatized_words)\n",
    "\t\t\t\n",
    "\t\texcept Exception as e: \n",
    "\t\t\tlogger.error(f'Error during lemmatizaion: {str(e)}')\n",
    "\t\t\treturn self.text\n",
    "\n",
    "\tdef __text_normalization(self) -> str: \n",
    "\t\t\"\"\"\n",
    "\t\tВыбирает метод нормализации текста. \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmatch self.normalization_method: \n",
    "\t\t\tcase 'lemma': \n",
    "\t\t\t\treturn self.__lemming()\n",
    "\t\t\tcase 'stem': \n",
    "\t\t\t\treturn self.__stemming()\n",
    "\t\t\tcase _: \n",
    "\t\t\t\tlogger.warning('There is no such thing as a text normalization method')\n",
    "\t\t\t\tlogger.info('Returned text without normalization')\n",
    "\t\t\t\treturn self.text\n",
    "\n",
    "\tdef process_text(self) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tОбрабатывает текст, применяя все методы предобработки.\n",
    "\t\t\n",
    "\t\t:return: обработанный текст по всем функциям.\n",
    "\t\t\"\"\" \n",
    "\n",
    "\t\tfor func in [self.__to_lower_case, self.__delete_punctuation_marks, self.__delete_stop_words, self.__text_normalization]: \n",
    "\t\t\t\tself.text = func()\n",
    "\t\t\n",
    "\t\tlogger.info('Text preprocessing is done!')\n",
    "\n",
    "\t\treturn self.text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:43:07,459 - INFO - Text is initializing...\n",
      "2025-03-22 23:43:07,464 - INFO - Detected language: russian\n",
      "2025-03-22 23:43:07,466 - INFO - Text is initialized with parameters: delete_punctuation_mode=string, text_normalization_method=lemma\n",
      "2025-03-22 23:43:07,466 - INFO - Text has been converted to lower case.\n",
      "2025-03-22 23:43:07,468 - INFO - Punctuation are being deleting...\n",
      "2025-03-22 23:43:07,468 - INFO - Punctuation has been deleted.\n",
      "2025-03-22 23:43:07,470 - INFO - Stop words are being removed...\n",
      "2025-03-22 23:43:07,472 - INFO - Stop words have been removed.\n",
      "2025-03-22 23:43:07,473 - INFO - Lemmatizaion is being performed...\n",
      "2025-03-22 23:43:25,717 - INFO - Lemmatization has been performed.\n",
      "2025-03-22 23:43:25,722 - INFO - Text preprocessing is done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бывало когданибудь такой знать точно такой читать какуюнибудь книга прошествие несколько глава вспомнить мочь прочитывать один дело это художественный литература другой научнопопулярный книга который какойто степень зависеть ваш ученый степень карьера конец конец\n"
     ]
    }
   ],
   "source": [
    "output = TextPreprocessing(\"Бывало ли у вас когда-нибудь такое, не знаю как у вас, но у меня точно такое было -- я читаю какую-нибудь книгу и по прошествии нескольких глав я и вспомнить не могу, о чем прочитал? Одно дело это с художественной литературой, а совсем другое с научно-популярными книгами, от которой, в какой-то степени может зависеть ваша учёная степень ну или карьера в конце концов.\", delete_punctuation_mode='string', text_normalization_method='lemma').process_text()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:43:25,729 - INFO - Text is initializing...\n",
      "2025-03-22 23:43:25,732 - INFO - Detected language: english\n",
      "2025-03-22 23:43:25,733 - INFO - Text is initialized with parameters: delete_punctuation_mode=string, text_normalization_method=lemma\n",
      "2025-03-22 23:43:25,734 - INFO - Text has been converted to lower case.\n",
      "2025-03-22 23:43:25,736 - INFO - Punctuation are being deleting...\n",
      "2025-03-22 23:43:25,736 - INFO - Punctuation has been deleted.\n",
      "2025-03-22 23:43:25,737 - INFO - Stop words are being removed...\n",
      "2025-03-22 23:43:25,738 - INFO - Stop words have been removed.\n",
      "2025-03-22 23:43:25,738 - INFO - Lemmatizaion is being performed...\n",
      "2025-03-22 23:43:25,740 - INFO - Lemmatization has been performed.\n",
      "2025-03-22 23:43:25,741 - INFO - Text preprocessing is done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfortunately pymorphy2 design specifically morphological analysis russian support english lemmatize english text use tool\n"
     ]
    }
   ],
   "source": [
    "eng_output = TextPreprocessing('Unfortunately, pymorphy2 is designed specifically for morphological analysis of Russian and does not support English. To lemmatize English text you should use other tools.').process_text()\n",
    "\n",
    "print(eng_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
